{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Ευριπίδης Παντελαίος - 1115201600124 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>Some useful functions </b><br>\n",
    "<b> 1) Cleaning</b><br>\n",
    "<b> 2) Lemmatization</b><br>\n",
    "<b> 3) Remove stop words </b><br>\n",
    "<b> 4) Part-of-Speech Tag</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data and remove symbols, urls, unnecessary words\n",
    "def cleanData(comments):\n",
    "    \n",
    "    StoredComments = []\n",
    "    for line in comments:\n",
    "      \n",
    "        line = line.lower()\n",
    "        \n",
    "        #replace some words, symbols and letters that appear frequently and are useless\n",
    "        line = line.replace('-', '')\n",
    "        line = line.replace('_', '')\n",
    "        line = line.replace('0', '')\n",
    "        line = line.replace(\"\\n\", '')\n",
    "        line = line.replace(\"\\\\\", '')\n",
    "        line = line.replace('XD', '')  \n",
    "        line = line.replace('..', '') \n",
    "        line = line.replace('  ', ' ') \n",
    "        line = line.replace('https', '')\n",
    "        line = line.replace('http', '')\n",
    "                                                                    \n",
    "        removeList = ['@', r'\\x', '\\\\', 'corrup', '^', '#', '$', '%', '&']\n",
    "        #for line in comments:\n",
    "        words = ' '.join([word for word in line.split() if not any([phrase in word for phrase in removeList]) ])\n",
    "        StoredComments.append(words)\n",
    "           \n",
    "    return StoredComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize the comments\n",
    "def lemmatizer (comments):\n",
    "    \n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    StoredComments = []\n",
    "    for line in comments:\n",
    "        line = ' '.join([lemma.lemmatize(w) for w in nltk.word_tokenize(line)])\n",
    "        StoredComments.append(line)\n",
    "        \n",
    "    return StoredComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "def removeStopWords (comments):\n",
    "    \n",
    "    StoredComments=[]\n",
    "    for line in comments:\n",
    "        line = ' '.join([w for w in nltk.word_tokenize(line) if w not in stop_words])\n",
    "        StoredComments.append(line)\n",
    "        \n",
    "    return StoredComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Pos tags and the frequency of them\n",
    "def posTag(comments):\n",
    "    \n",
    "    adjectiveFrequency=[]\n",
    "    adverbFrequency=[]\n",
    "    nounFrequency=[]\n",
    "    verbFrequency=[]\n",
    "    \n",
    "    for comment in comments:\n",
    "        \n",
    "        adjectiveCounter=0\n",
    "        adverbCounter=0\n",
    "        nounCounter=0\n",
    "        verbCounter=0\n",
    "        \n",
    "        #Pos tagging the words \n",
    "        words = nltk.word_tokenize(comment)\n",
    "        words = nltk.pos_tag(words)\n",
    "        cnt = len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            if(word[1][:1] == 'NN'):\n",
    "                nounCounter = nounCounter+1\n",
    "            \n",
    "            elif(word[1][:1] == 'VV'):\n",
    "                verbCounter = verbCounter+1 \n",
    "                \n",
    "            elif(word[1][:1] == 'RR'):\n",
    "                adverbCounter = adverbCounter+1\n",
    "                \n",
    "            elif(word[1][:1] == 'JJ'):\n",
    "                adjectiveCounter = adjectiveCounter+1\n",
    "                \n",
    "                \n",
    "        #not divide with zero       \n",
    "        if(cnt!=0):    #calculate the frequency of each tag\n",
    "            nounFrequency.append(nounCounter/cnt)\n",
    "            verbFrequency.append(verbCounter/cnt)\n",
    "            adverbFrequency.append(adverbCounter/cnt)\n",
    "            adjectiveFrequency.append(adjectiveCounter/cnt)\n",
    "            \n",
    "        else:\n",
    "            nounFrequency.append(0)\n",
    "            verbFrequency.append(0)\n",
    "            adverbFrequency.append(0)\n",
    "            adjectiveFrequency.append(0)\n",
    "            \n",
    "    return nounFrequency, verbFrequency, adverbFrequency, adjectiveFrequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b> Read csv files for train and test set and cleaning the data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = pd.read_csv(\"data/train.csv\")\n",
    "testSet = pd.read_csv(\"data/impermium_verification_labels.csv\")   #I dont use the file 'impermium_verification_set.csv' at all, \n",
    "                                                                    #because the other file named 'impermium_verification_labels.csv'\n",
    "                                                                    #covers completely the requirements of the exercise.\n",
    "\n",
    "#Cleaning the data and test set\n",
    "trainSet['Comment'] = cleanData(trainSet['Comment'])\n",
    "testSet['Comment'] = cleanData(testSet['Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>Train the train data with Bag of Words </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer()\n",
    "\n",
    "BagOfWordsTrain = countVectorizer.fit_transform(trainSet['Comment'].values)\n",
    "BagOfWordsTrainArray = BagOfWordsTrain.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>Train the test data with Bag of Words </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BagOfWordsTest = countVectorizer.transform(testSet['Comment'].values)\n",
    "BagOfWordsTestArray = BagOfWordsTest.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b> Gaussian Naive Bayes classifier </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierNB = GaussianNB()\n",
    "\n",
    "classifierNB.fit(BagOfWordsTrainArray, trainSet['Insult'])\n",
    "\n",
    "BoWprediction = classifierNB.predict(BagOfWordsTestArray)\n",
    "\n",
    "y_test = testSet['Insult']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b> Gaussian Naive Bayes Scores</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5266219239373602\n",
      "F1 Score: 0.5208333333333333\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy Score:', accuracy_score(y_test, BoWprediction))\n",
    "print('F1 Score:', f1_score(y_test, BoWprediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b> Now I am doing 4 optimizations for Naive Bayes (Lemmatization, Remove stop words, Bigrams, Laplace Smoothing</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1) Lemmatization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet['commentLemmatization'] = lemmatizer(trainSet['Comment'])\n",
    "testSet['commentLemmatization'] = lemmatizer(testSet['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5257270693512305\n",
      "F1 Score: 0.5276292335115864\n"
     ]
    }
   ],
   "source": [
    "lemmazationTrain = countVectorizer.fit_transform(trainSet['commentLemmatization'])\n",
    "lemmazationTrainArray = lemmazationTrain.toarray()\n",
    "\n",
    "lemmazationTest = countVectorizer.transform(testSet['commentLemmatization'])\n",
    "lemmazationTestArray = lemmazationTest.toarray()\n",
    "\n",
    "classifierNB.fit(lemmazationTrainArray,trainSet['Insult'])\n",
    "lemmatizationPredict = classifierNB.predict(lemmazationTestArray)\n",
    "\n",
    "print('Accuracy Score:', accuracy_score(y_test, lemmatizationPredict))\n",
    "print('F1 Score:', f1_score(y_test, lemmatizationPredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>2) Remove stop words </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet['commentStopWords'] = removeStopWords(trainSet['Comment'])\n",
    "testSet['commentStopWords'] = removeStopWords(testSet['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5243847874720358\n",
      "F1 Score: 0.5174761688606445\n"
     ]
    }
   ],
   "source": [
    "stopWordsTrain = countVectorizer.fit_transform(trainSet['commentStopWords'])\n",
    "stopWordsTrainArray = stopWordsTrain.toarray()\n",
    "\n",
    "stopWordsTest = countVectorizer.transform(testSet['commentStopWords'])\n",
    "stopWordsTestArray = stopWordsTest.toarray()\n",
    "\n",
    "classifierNB.fit(stopWordsTrainArray,trainSet['Insult'])\n",
    "stopWordPredict = classifierNB.predict(stopWordsTestArray)\n",
    "\n",
    "print ('Accuracy Score:', accuracy_score(y_test, stopWordPredict))\n",
    "print('F1 Score:', f1_score(y_test, stopWordPredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b> 3) Bigrams</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.556599552572707\n",
      "F1 Score: 0.5292161520190024\n"
     ]
    }
   ],
   "source": [
    "bigramVectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "bigramTrain = bigramVectorizer.fit_transform(trainSet['Comment'])\n",
    "bigramTrainArray = bigramTrain.toarray()\n",
    "\n",
    "bigramTest= bigramVectorizer.transform(testSet['Comment'])\n",
    "bigramTestArray = bigramTest.toarray()\n",
    "\n",
    "classifierNB.fit(bigramTrainArray,trainSet['Insult'])\n",
    "bigramPredict = classifierNB.predict(bigramTestArray)\n",
    "\n",
    "print ('Accuracy Score:', accuracy_score(y_test, bigramPredict))\n",
    "print('F1 Score:', f1_score(y_test, bigramPredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b> 4) Laplace Smoothing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6769574944071588\n",
      "F1 Score: 0.6143162393162394\n"
     ]
    }
   ],
   "source": [
    "classifierMultinomialNB = MultinomialNB(alpha=1.0)\n",
    "classifierMultinomialNB.fit(BagOfWordsTrainArray,trainSet['Insult'])\n",
    "laplacePredict = classifierMultinomialNB.predict(BagOfWordsTestArray)\n",
    "\n",
    "print ('Accuracy Score:', accuracy_score(y_test, laplacePredict))\n",
    "print('F1 Score:', f1_score(y_test, laplacePredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> <b>Tf-idf Vectorizer </b> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfIdf = TfidfVectorizer()\n",
    "\n",
    "TfIdfTrain = TfIdf.fit_transform(trainSet['Comment'])\n",
    "TfIdfTest = TfIdf.transform(testSet['Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> <b>Part-of-Speech features for Train set </b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdjectiveTrain, AdverbTrain, NounTrain, VerbTrain = posTag(trainSet['Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>Append tf-idf and Part-of-Speech features for train set</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "posTrainVectorizer = scipy.sparse.hstack((TfIdfTrain, scipy.sparse.csr_matrix(NounTrain).T))\n",
    "\n",
    "posTrainVectorizer = scipy.sparse.hstack((posTrainVectorizer, scipy.sparse.csr_matrix(AdjectiveTrain).T))\n",
    "\n",
    "posTrainVectorizer = scipy.sparse.hstack((posTrainVectorizer, scipy.sparse.csr_matrix(AdverbTrain).T))\n",
    "\n",
    "posTrainVectorizer = scipy.sparse.hstack((posTrainVectorizer, scipy.sparse.csr_matrix(VerbTrain).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b>Part-of-Speech features for Test set </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdjectiveTest, AdverbTest, NounTest, VerbTest = posTag(testSet['Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>Append tf-idf and Part-of-Speech features for test set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "posTestVectorizer = scipy.sparse.hstack((TfIdfTest, scipy.sparse.csr_matrix(NounTest).T))\n",
    "\n",
    "posTestVectorizer = scipy.sparse.hstack((posTestVectorizer, scipy.sparse.csr_matrix(AdjectiveTest).T))\n",
    "\n",
    "posTestVectorizer = scipy.sparse.hstack((posTestVectorizer, scipy.sparse.csr_matrix(AdverbTest).T))\n",
    "\n",
    "posTestVectorizer = scipy.sparse.hstack((posTestVectorizer, scipy.sparse.csr_matrix(VerbTest).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b> Test score for Tf-idf PoS model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.545413870246085\n",
      "F1 Score: 0.11343804537521815\n"
     ]
    }
   ],
   "source": [
    "classifierMultinomialNB.fit(posTrainVectorizer, trainSet['Insult'])\n",
    "posVectorizerPredict = classifierMultinomialNB.predict(posTestVectorizer)\n",
    "\n",
    "print('Accuracy Score:', accuracy_score(y_test, posVectorizerPredict))\n",
    "print('F1 Score:', f1_score(y_test, posVectorizerPredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b>SVM </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1.0, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6926174496644295\n",
      "Test F1: 0.6094371802160318\n"
     ]
    }
   ],
   "source": [
    "svc.fit(posTrainVectorizer,trainSet['Insult']) \n",
    "posVectorizerSVM = svc.predict(posTestVectorizer)\n",
    "\n",
    "print ('Accuracy Score:', accuracy_score(y_test, posVectorizerSVM))\n",
    "print ('Test F1:', f1_score(y_test, posVectorizerSVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b> Random Decision Forest</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6259507829977629\n",
      "Test F1: 0.42024965325936203\n"
     ]
    }
   ],
   "source": [
    "randomDecisionForest = RandomForestClassifier(n_estimators = 150)\n",
    "\n",
    "randomDecisionForest.fit(posTrainVectorizer, trainSet['Insult']) \n",
    "posVectorizerRandomForest = randomDecisionForest.predict(posTestVectorizer)\n",
    "\n",
    "print ('Accuracy Score:', accuracy_score(y_test, posVectorizerRandomForest))\n",
    "print ('Test F1:', f1_score(y_test, posVectorizerRandomForest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b> Beat the benchmark with proper data processing with lemmatization, remove stop words and using Tf-idf and SVM</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6917225950782998\n",
      "F1 Score: 0.6005797101449276\n"
     ]
    }
   ],
   "source": [
    "#I couldn't improve the scores much ...\n",
    "#as there are many slang words and methods that are impossible to understand,\n",
    "#even with modern improved algorithms,  if these words are offensive or not.\n",
    "#If the values of dataset were labeled correct I could produce better results.\n",
    "\n",
    "TfIdf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "trainSet['commentLemmatization'] = removeStopWords(trainSet['commentLemmatization'])\n",
    "testSet['commentLemmatization'] = removeStopWords(testSet['commentLemmatization'])\n",
    "\n",
    "TfIdfTrain = TfIdf.fit_transform(trainSet['commentLemmatization'])\n",
    "TfIdfTest = TfIdf.transform(testSet['commentLemmatization'])\n",
    "\n",
    "svc.fit(TfIdfTrain,trainSet['Insult'])\n",
    "TfIdfPredict = svc.predict(TfIdfTest)\n",
    "\n",
    "print ('Accuracy Score:', accuracy_score(y_test, TfIdfPredict))\n",
    "print ('F1 Score:', f1_score(y_test, TfIdfPredict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
